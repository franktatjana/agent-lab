# Synthesize Learnings

Turn user testing results into actionable next steps. Decide whether to iterate, pivot, or proceed based on evidence.

## When to use

After running user tests on a prototype. The team has raw feedback and needs to decide what it means and what to do next.

## Instructions

Given test session notes and observations:

1. **Pattern extraction**: What feedback appeared across multiple participants? What was unique to one?
2. **Assumption validation**: For each risky assumption tested:
   - **Validated**: Evidence supports it (what evidence?)
   - **Invalidated**: Evidence contradicts it (what evidence?)
   - **Inconclusive**: Not enough signal (what would you need?)
3. **Surprise capture**: What did users do or say that nobody expected?
4. **Decide next step**:
   - **Iterate**: The concept works but needs refinement. What specifically changes?
   - **Pivot**: The core assumption failed. What new direction does the evidence suggest?
   - **Proceed**: Key assumptions validated. What does the next stage look like?
5. **Update problem framing**: Does the original HMW question still hold, or has the team learned something that changes the problem definition?

## Output

- Assumption scorecard: validated / invalidated / inconclusive with evidence
- Key surprises and unexpected findings
- Decision: iterate / pivot / proceed with rationale
- Updated action plan: specific changes or next steps
- Revised problem framing (if applicable)
